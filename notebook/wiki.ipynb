{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Download Main Entry Categories and Pages Manually",
   "id": "6a05782b57715785"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T13:52:11.948371Z",
     "start_time": "2025-03-24T13:52:11.679984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import urllib.parse"
   ],
   "id": "4841cde023e6accf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T13:52:13.332725Z",
     "start_time": "2025-03-24T13:52:12.866786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_links_from_mp_other_content():\n",
    "    \"\"\"\n",
    "    Extract all links from the mp-other-content div on the Burmese Wikipedia main page\n",
    "    that start with https://my.wikipedia.org/wiki/\n",
    "    \"\"\"\n",
    "    # URL of the Burmese Wikipedia main page\n",
    "    url = \"https://my.wikipedia.org/wiki/%E1%80%97%E1%80%9F%E1%80%AD%E1%80%AF%E1%80%85%E1%80%AC%E1%80%99%E1%80%BB%E1%80%80%E1%80%BA%E1%80%94%E1%80%BE%E1%80%AC\"\n",
    "\n",
    "    # Send a request to get the HTML content\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.encoding = 'utf-8'  # Ensure proper encoding for Burmese characters\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return f\"Error: Failed to retrieve the page (Status Code: {response.status_code})\"\n",
    "\n",
    "    print(\"Successfully retrieved the Wikipedia page\")\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the div with id=\"mp-other-content\"\n",
    "    mp_other_content = soup.find('div', id='mp-other-content')\n",
    "\n",
    "    if not mp_other_content:\n",
    "        return \"Error: Could not find the div with id='mp-other-content' on the page.\"\n",
    "\n",
    "    print(\"Found the mp-other-content div\")\n",
    "\n",
    "    # Extract all links from this div\n",
    "    page_links = []\n",
    "    category_links = []\n",
    "    header = \"\"\n",
    "    for link in mp_other_content.find_all('a'):\n",
    "        href = link.get('href', '')\n",
    "        text = link.get_text(strip=True)\n",
    "        title = link.get('title', '')\n",
    "\n",
    "        # Get the full HTML content of the link\n",
    "        link_html = str(link)\n",
    "        # Check if the link contains a <b> tag\n",
    "        has_bold = '<b>' in link_html\n",
    "        if has_bold:\n",
    "            header = title.replace(\"ကဏ္ဍ:\", \"\")\n",
    "\n",
    "\n",
    "\n",
    "        # Convert relative URLs to absolute URLs\n",
    "        if href.startswith('/wiki/'):\n",
    "            full_url = f\"https://my.wikipedia.org{href}\"\n",
    "        else:\n",
    "            full_url = href\n",
    "\n",
    "        # Check if the URL starts with https://my.wikipedia.org/wiki/\n",
    "        if full_url.startswith(\"https://my.wikipedia.org/wiki/%E1%80%80%E1%80%8F%E1%80%B9%E1%80%8D:\"):\n",
    "            category_link_urls = [cat['url'] for cat in category_links]\n",
    "            if full_url not in category_link_urls:\n",
    "                category_links.append({\n",
    "                    'header': header,\n",
    "                    'text': title.replace(\"ကဏ္ဍ:\", \"\"),\n",
    "                    'title': title,\n",
    "                    'url': full_url\n",
    "                })\n",
    "        elif full_url.startswith('https://my.wikipedia.org/wiki/'):\n",
    "            link_urls = [cat['url'] for cat in page_links]\n",
    "            if full_url not in link_urls:\n",
    "                page_links.append({\n",
    "                    'header': header,\n",
    "                    'text': text,\n",
    "                    'title': title,\n",
    "                    'url': full_url\n",
    "                })\n",
    "\n",
    "\n",
    "    print(f\"Found {len(page_links)} links starting with https://my.wikipedia.org/wiki/\")\n",
    "    print(f\"Found {len(category_links)} category_links starting with https://my.wikipedia.org/wiki/%E1%80%80%E1%80%8F%E1%80%B9%E1%80%8D:\")\n",
    "\n",
    "    return {\n",
    "        'page_links': page_links,\n",
    "        'category_links': category_links,\n",
    "        'total_links_found': len(page_links) + len(category_links)\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = extract_links_from_mp_other_content()\n",
    "\n",
    "    # Print the result in a readable format\n",
    "    print(\"\\n===== RESULTS =====\")\n",
    "\n",
    "    if isinstance(result, str):\n",
    "        print(result)  # Error message\n",
    "    else:\n",
    "        print(f\"Found {result['total_links_found']} links in mp-other-content:\")\n",
    "\n",
    "        # Save to a JSON file\n",
    "        with open('../data/myanmar_wiki_entry_links.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(\"\\nResults also saved to burmese_wiki_links.json\")"
   ],
   "id": "e39f1524a7650602",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved the Wikipedia page\n",
      "Found the mp-other-content div\n",
      "Found 36 links starting with https://my.wikipedia.org/wiki/\n",
      "Found 65 category_links starting with https://my.wikipedia.org/wiki/%E1%80%80%E1%80%8F%E1%80%B9%E1%80%8D:\n",
      "\n",
      "===== RESULTS =====\n",
      "Found 101 links in mp-other-content:\n",
      "\n",
      "Results also saved to burmese_wiki_links.json\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T11:47:41.101463Z",
     "start_time": "2025-03-24T11:47:41.099084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"../data/myanmar_wiki_entry_links.json\") as file:\n",
    "    myanmar_wiki_links = json.load(file)\n",
    "\n",
    "categories = myanmar_wiki_links['category_links']\n",
    "pages = myanmar_wiki_links['page_links']\n",
    "categories = [[category['text']] for category in categories]"
   ],
   "id": "cf1b9339545b0ee0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T17:27:13.687918Z",
     "start_time": "2025-03-15T17:27:13.686559Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ca0df640d2fe85d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Get the pages from main entry link with url",
   "id": "1f3fe28012126e41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T11:47:57.708468Z",
     "start_time": "2025-03-24T11:47:57.706676Z"
    }
   },
   "cell_type": "code",
   "source": "base_url = \"https://my.wikipedia.org/w/api.php\"",
   "id": "67d9dcd6cf2e653d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T11:48:00.961296Z",
     "start_time": "2025-03-24T11:48:00.958007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_page_by_url(url):\n",
    "    \"\"\"\n",
    "    Get a Wikipedia page by its URL\n",
    "\n",
    "    Args:\n",
    "        url (str): Full Wikipedia page URL\n",
    "\n",
    "    Returns:\n",
    "        dict: Page content information or None if not found\n",
    "    \"\"\"\n",
    "    # Extract the page title from the URL\n",
    "\n",
    "    # Parse the URL\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "\n",
    "    # Get the path component and remove the '/wiki/' prefix\n",
    "    path = parsed_url.path\n",
    "\n",
    "    if path.startswith('/wiki/'):\n",
    "        # Extract the title and decode it\n",
    "        page_title = urllib.parse.unquote(path.replace(\"/wiki/\", \"\"))\n",
    "    else:\n",
    "        raise f\"Link has issue: {url}\"\n",
    "\n",
    "    # Now use the API to get the page by title\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'titles': page_title,\n",
    "        'prop': 'extracts|categories|info',\n",
    "        'explaintext': True,\n",
    "        'exsectionformat': 'plain',\n",
    "        'inprop': 'url',\n",
    "        'format': 'json',\n",
    "        'redirects': True\n",
    "    }\n",
    "\n",
    "    response = requests.get(url=base_url, params=params).json()\n",
    "\n",
    "    if 'query' in response and 'pages' in response['query']:\n",
    "        # The API returns a dict with page IDs as keys\n",
    "        # We don't know the page ID in advance, so we get the first (and only) page\n",
    "        pages = response['query']['pages']\n",
    "\n",
    "        # Check if the page exists (page ID -1 means it doesn't exist)\n",
    "        if '-1' in pages:\n",
    "            raise f\"ID has issue: {url}\"\n",
    "\n",
    "        page_id = next(iter(pages))\n",
    "        page_data = pages[page_id]\n",
    "\n",
    "        return {\n",
    "            'title': page_data.get('title', ''),\n",
    "            'content': page_data.get('extract', ''),\n",
    "            'categories': [cat['title'] for cat in page_data.get('categories', [])],\n",
    "            'url': page_data.get('fullurl', ''),\n",
    "            'last_modified': page_data.get('touched', ''),\n",
    "        }\n",
    "    else:\n",
    "        raise f\"Response has issue: {url}\"\n"
   ],
   "id": "6004e3bea6c0a5ab",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T11:48:03.550842Z",
     "start_time": "2025-03-24T11:48:02.480786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "page_data = get_page_by_url(\"https://my.wikipedia.org/wiki/%E1%80%80%E1%80%99%E1%80%B9%E1%80%98%E1%80%AC\")\n",
    "df = pd.DataFrame([page_data])\n",
    "df"
   ],
   "id": "92cae3383cc843ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         title                                            content  \\\n",
       "0  ကမ္ဘာဂြိုဟ်  \"\" ဖြင့် နက္ခတ္တဗေဒတွင် သင်္ကေတပြုရိုးရှိသည့် ...   \n",
       "\n",
       "                                          categories  \\\n",
       "0  [ကဏ္ဍ:CS1 maint: multiple names: editors list,...   \n",
       "\n",
       "                                                 url         last_modified  \n",
       "0  https://my.wikipedia.org/wiki/%E1%80%80%E1%80%...  2025-03-19T16:53:42Z  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>categories</th>\n",
       "      <th>url</th>\n",
       "      <th>last_modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ကမ္ဘာဂြိုဟ်</td>\n",
       "      <td>\"\" ဖြင့် နက္ခတ္တဗေဒတွင် သင်္ကေတပြုရိုးရှိသည့် ...</td>\n",
       "      <td>[ကဏ္ဍ:CS1 maint: multiple names: editors list,...</td>\n",
       "      <td>https://my.wikipedia.org/wiki/%E1%80%80%E1%80%...</td>\n",
       "      <td>2025-03-19T16:53:42Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T11:48:07.818171Z",
     "start_time": "2025-03-24T11:48:07.816747Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ae18d1ca8e1c6b80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T11:48:08.637806Z",
     "start_time": "2025-03-24T11:48:08.636420Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b345d7a6eb3f4bd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Get the pages and categories for a given category",
   "id": "a77b94a35158f0ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T11:48:14.293841Z",
     "start_time": "2025-03-24T11:48:14.291452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import pandas as pd\n",
    "# import time\n",
    "#\n",
    "# session = requests.Session()\n",
    "#\n",
    "# def get_category_members(category):\n",
    "#     # Add Category: prefix if not present\n",
    "#     if not category.startswith('Category:'):\n",
    "#         category = f\"Category:{category}\"\n",
    "#\n",
    "#     pages = []\n",
    "#     subcategories = []\n",
    "#\n",
    "#     params = {\n",
    "#         'action': 'query',\n",
    "#         'list': 'categorymembers',\n",
    "#         'cmtitle': category,\n",
    "#         'cmtype': 'page|subcat',\n",
    "#         'cmlimit': 500,\n",
    "#         'format': 'json'\n",
    "#     }\n",
    "#\n",
    "#     continuation = True\n",
    "#\n",
    "#     while continuation:\n",
    "#         response = session.get(url=base_url, params=params).json()\n",
    "#\n",
    "#         if 'error' in response:\n",
    "#             print(f\"Error: {response['error']['info']}\")\n",
    "#             break\n",
    "#\n",
    "#         if 'query' not in response:\n",
    "#             print(f\"No results found for {category}\")\n",
    "#             break\n",
    "#\n",
    "#         members = response['query']['categorymembers']\n",
    "#\n",
    "#         for member in members:\n",
    "#             ns = member['ns']\n",
    "#             title = member['title']\n",
    "#\n",
    "#             if ns == 14:  # Namespace 14 is for categories\n",
    "#                 subcategories.append(title)\n",
    "#             else:\n",
    "#                 pages.append({\n",
    "#                     'title': title,\n",
    "#                     'pageid': member['pageid']\n",
    "#                 })\n",
    "#\n",
    "#         if 'continue' in response:\n",
    "#             params['cmcontinue'] = response['continue']['cmcontinue']\n",
    "#         else:\n",
    "#             continuation = False\n",
    "#\n",
    "#     return pages, subcategories\n",
    "#\n",
    "# def get_page_content(page_id):\n",
    "#     params = {\n",
    "#         'action': 'query',\n",
    "#         'prop': 'extracts|categories|info',\n",
    "#         'pageids': page_id,\n",
    "#         'explaintext': True,\n",
    "#         'exsectionformat': 'plain',\n",
    "#         'inprop': 'url',\n",
    "#         'format': 'json'\n",
    "#     }\n",
    "#\n",
    "#     response = session.get(url=base_url, params=params).json()\n",
    "#\n",
    "#     if 'query' in response and 'pages' in response['query']:\n",
    "#         page_data = response['query']['pages'][str(page_id)]\n",
    "#         return {\n",
    "#             'title': page_data.get('title', ''),\n",
    "#             'content': page_data.get('extract', ''),\n",
    "#             'categories': [cat['title'] for cat in page_data.get('categories', [])],\n",
    "#             'url': page_data.get('fullurl', ''),\n",
    "#             'last_modified': page_data.get('touched', '')\n",
    "#         }\n",
    "#     else:\n",
    "#         return None\n",
    "#\n",
    "# def fetch_pages_and_subcategories(category):\n",
    "#     all_pages = []\n",
    "#\n",
    "#     pages, subcategories = get_category_members(category[-1])\n",
    "#\n",
    "#     print(\"Pages:\", len(pages))\n",
    "#     print(\"Subcategories:\", len(subcategories))\n",
    "#\n",
    "#     # Process pages\n",
    "#     for page in pages:\n",
    "#         print(f\"Fetching page: {page['title']}\")\n",
    "#         page_data = get_page_content(page['pageid'])\n",
    "#\n",
    "#         if page_data and page_data['content']:\n",
    "#             page_data['source_category'] = category\n",
    "#             all_pages.append(page_data)\n",
    "#\n",
    "#         # Be gentle with the API\n",
    "#         time.sleep(1)\n",
    "#\n",
    "#     subcategories = [subcategory.replace(\"ကဏ္ဍ:\", \"\") for subcategory in subcategories]\n",
    "#\n",
    "#     # Convert to DataFrame\n",
    "#     if all_pages:\n",
    "#         return pd.DataFrame(all_pages), subcategories\n",
    "#     else:\n",
    "#         return pd.DataFrame(columns=['title', 'content', 'categories', 'url', 'last_modified', 'source_category']), subcategories"
   ],
   "id": "8f0df3b969c85d14",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T13:14:08.402447Z",
     "start_time": "2025-03-24T13:14:08.029636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "def get_category_members(category):\n",
    "    # Add Category: prefix if not present\n",
    "    if not category.startswith(('Category:', 'ကဏ္ဍ:')):\n",
    "        category = f\"Category:{category}\"\n",
    "\n",
    "    pages = []\n",
    "    subcategories = []\n",
    "\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'list': 'categorymembers',\n",
    "        'cmtitle': category,\n",
    "        'cmtype': 'page|subcat',\n",
    "        'cmlimit': 500,\n",
    "        'format': 'json'\n",
    "    }\n",
    "\n",
    "    continuation = True\n",
    "\n",
    "\n",
    "\n",
    "    max_retries = 100\n",
    "    response = None\n",
    "    for attempt in range(max_retries):\n",
    "        if not continuation:\n",
    "            return pages, subcategories\n",
    "\n",
    "        try:\n",
    "            while continuation:\n",
    "                response = session.get(url=base_url, params=params).json()\n",
    "\n",
    "                if 'error' in response:\n",
    "                    print(f\"Error: {response['error']['info']}\")\n",
    "                    break\n",
    "\n",
    "                if response is None:\n",
    "                    print(f\"Timeout: {category}\")\n",
    "                    break\n",
    "\n",
    "                if 'query' not in response:\n",
    "                    print(f\"No results found for {category}\")\n",
    "                    break\n",
    "\n",
    "                members = response['query']['categorymembers']\n",
    "\n",
    "                for member in members:\n",
    "                    ns = member['ns']\n",
    "                    title = member['title']\n",
    "\n",
    "                    if ns == 14:  # Namespace 14 is for categories\n",
    "                        subcategories.append(title)\n",
    "                    else:\n",
    "                        pages.append({\n",
    "                            'title': title,\n",
    "                            'pageid': member['pageid']\n",
    "                        })\n",
    "\n",
    "                if 'continue' in response:\n",
    "                    params['cmcontinue'] = response['continue']['cmcontinue']\n",
    "                else:\n",
    "                    continuation = False\n",
    "\n",
    "        except (requests.exceptions.RequestException, ValueError) as e:\n",
    "            wait_time = (attempt + 1) * 5  # Increasing backoff\n",
    "            logger.warning(f\"Error fetching category {category}. Retrying in {wait_time}s... ({attempt+1}/{max_retries})\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "def get_page_content(page_id, session=None):\n",
    "    # Create a new session if none is provided\n",
    "    local_session = session or requests.Session()\n",
    "\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'prop': 'extracts|categories|info',\n",
    "        'pageids': page_id,\n",
    "        'explaintext': True,\n",
    "        'exsectionformat': 'plain',\n",
    "        'inprop': 'url',\n",
    "        'format': 'json'\n",
    "    }\n",
    "\n",
    "    max_retries = 100\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = local_session.get(url=base_url, params=params).json()\n",
    "            if 'query' in response and 'pages' in response['query']:\n",
    "                page_data = response['query']['pages'][str(page_id)]\n",
    "                return {\n",
    "                    'title': page_data.get('title', ''),\n",
    "                    'content': page_data.get('extract', ''),\n",
    "                    'categories': [cat['title'] for cat in page_data.get('categories', [])],\n",
    "                    'url': page_data.get('fullurl', ''),\n",
    "                    'last_modified': page_data.get('touched', '')\n",
    "                }\n",
    "            else:\n",
    "                return None\n",
    "        except (requests.exceptions.RequestException, ValueError) as e:\n",
    "            wait_time = (attempt + 1) * 5  # Increasing backoff\n",
    "            logger.warning(f\"Error fetching page {page_id}: {e}. Retrying in {wait_time}s... ({attempt+1}/{max_retries})\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "def process_page(page, category, session=None):\n",
    "    page_data = get_page_content(page['pageid'], session)\n",
    "\n",
    "    if page_data and page_data['content']:\n",
    "        page_data['source_category'] = category\n",
    "        return page_data\n",
    "    return None\n",
    "\n",
    "def process_batch(batch, category):\n",
    "    \"\"\"Process a batch of pages\"\"\"\n",
    "    results = []\n",
    "    for page in batch:\n",
    "        result = process_page(page, category)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "def fetch_pages_and_subcategories(category):\n",
    "    _, categories = get_category_info(category[-1])\n",
    "    pages, subcategories = get_category_members(category[-1])\n",
    "\n",
    "    if len(pages) == 0:\n",
    "        return pd.DataFrame(columns=['title', 'content', 'categories', 'url', 'last_modified', 'source_category']), subcategories + categories\n",
    "\n",
    "    num_processes = 10\n",
    "    batch_size = min(num_processes, len(pages))\n",
    "\n",
    "    # num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "    logger.info(f\"Category: {category[-1]}, Pages: {len(pages)}, Subcategories: {len(subcategories)}, Categories: {len(categories)}\")\n",
    "\n",
    "    all_results = []\n",
    "    total_batches = (len(pages) + batch_size - 1) // batch_size\n",
    "    for i in tqdm(range(0, len(pages), batch_size), total=total_batches, desc=\"Processing batches\"):\n",
    "        batch = pages[i:i+batch_size]\n",
    "\n",
    "        # Create a pool for each batch to ensure fresh connections\n",
    "        with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "            # Create a partial function with fixed arguments\n",
    "            process_page_partial = partial(process_page, category=category, session=session)\n",
    "\n",
    "            # Process the batch\n",
    "            results = pool.map(process_page_partial, batch)\n",
    "            all_results.extend(results)\n",
    "\n",
    "        # Filter out None results\n",
    "    all_pages = [result for result in all_results if result is not None]\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    if all_pages:\n",
    "        return pd.DataFrame(all_pages), subcategories + categories\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['title', 'content', 'categories', 'url', 'last_modified', 'source_category']), subcategories + categories\n"
   ],
   "id": "2bdb689bde5bc5b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "([], ['ကဏ္ဍ:ဒဿနိကဗေဒ ပညာရေး'])\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T12:56:56.495588Z",
     "start_time": "2025-03-24T12:56:56.492710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_page_redirect_title(title):\n",
    "    # Now use the API to get the page by title\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'titles': title,\n",
    "        'prop': 'extracts|categories|info',\n",
    "        'explaintext': True,\n",
    "        'exsectionformat': 'plain',\n",
    "        'inprop': 'url',\n",
    "        'format': 'json',\n",
    "        'redirects': True\n",
    "    }\n",
    "\n",
    "    max_retries = 100\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url=base_url, params=params).json()\n",
    "\n",
    "            if 'query' in response and 'pages' in response['query']:\n",
    "                # The API returns a dict with page IDs as keys\n",
    "                # We don't know the page ID in advance, so we get the first (and only) page\n",
    "                pages = response['query']['pages']\n",
    "\n",
    "                page_id = next(iter(pages))\n",
    "                page_data = pages[page_id]\n",
    "\n",
    "                return page_data.get('title', '')\n",
    "        except Exception as e:\n",
    "            wait_time = (attempt + 1) * 5  # Increasing backoff\n",
    "            logger.warning(f\"Error fetching category {category}. Retrying in {wait_time}s... ({attempt+1}/{max_retries})\")\n",
    "            time.sleep(wait_time)"
   ],
   "id": "ebd99477545425cf",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T13:21:55.136079Z",
     "start_time": "2025-03-24T13:21:54.782741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_category_info(category):\n",
    "    \"\"\"\n",
    "    Get details about a category including its normalized name and parent categories\n",
    "    Returns a tuple of (normalized_name, parent_categories)\n",
    "    \"\"\"\n",
    "    # Add Category: prefix if not present\n",
    "    if not category.startswith(('Category:', 'ကဏ္ဍ:')):\n",
    "        category = f\"Category:{category}\"\n",
    "\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'titles': category,\n",
    "        'prop': 'categories',\n",
    "        'cllimit': 500,\n",
    "        'format': 'json'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = session.get(url=base_url, params=params).json()\n",
    "\n",
    "        # Extract the normalized \"to\" name\n",
    "        normalized_name = None\n",
    "        if 'normalized' in response.get('query', {}):\n",
    "            for norm in response['query']['normalized']:\n",
    "                if norm.get('from') == category:\n",
    "                    normalized_name = norm.get('to')\n",
    "                    break\n",
    "\n",
    "        # If no normalization happened, use the original category\n",
    "        if normalized_name is None:\n",
    "            normalized_name = category\n",
    "\n",
    "        # Extract parent categories\n",
    "        parent_categories = []\n",
    "        if 'pages' in response.get('query', {}):\n",
    "            pages = response['query']['pages']\n",
    "\n",
    "            for page_id, page_info in pages.items():\n",
    "                if 'categories' in page_info:\n",
    "                    for cat in page_info['categories']:\n",
    "                        parent_categories.append(cat['title'])\n",
    "\n",
    "        return normalized_name, parent_categories\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting category info: {str(e)}\")\n",
    "        return category, []\n",
    "\n",
    "# Example usage\n",
    "to_name, categories = get_category_info('ဒဿနိကဗေဒနှင့်_လူ့အဖွဲ့အစည်း')\n",
    "print(f\"Normalized name: {to_name}\")\n",
    "print(f\"Parent categories: {categories}\")"
   ],
   "id": "c326e876c24650b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized name: ကဏ္ဍ:ဒဿနိကဗေဒနှင့် လူ့အဖွဲ့အစည်း\n",
      "Parent categories: ['ကဏ္ဍ:ဒဿနိကဗေဒ', 'ကဏ္ဍ:လူ့အဖွဲ့အစည်း']\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T13:35:59.243156Z",
     "start_time": "2025-03-24T13:35:59.240230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_category_redirect_name(category):\n",
    "    \"\"\"\n",
    "    Get details about a category including its normalized name and parent categories\n",
    "    Returns a tuple of (normalized_name, parent_categories)\n",
    "    \"\"\"\n",
    "    # Add Category: prefix if not present\n",
    "    if not category.startswith(('Category:', 'ကဏ္ဍ:')):\n",
    "        category = f\"Category:{category}\"\n",
    "\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'titles': category,\n",
    "        'prop': 'categories',\n",
    "        'cllimit': 500,\n",
    "        'format': 'json',\n",
    "        \"redirect\": True\n",
    "    }\n",
    "    max_retries = 100\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = session.get(url=base_url, params=params).json()\n",
    "\n",
    "            # Extract the normalized \"to\" name\n",
    "            normalized_name = None\n",
    "            if 'normalized' in response.get('query', {}):\n",
    "                for norm in response['query']['normalized']:\n",
    "                    if norm.get('from') == category:\n",
    "                        normalized_name = norm.get('to')\n",
    "                        break\n",
    "\n",
    "            # If no normalization happened, use the original category\n",
    "            if normalized_name is None:\n",
    "                normalized_name = category\n",
    "\n",
    "            return normalized_name\n",
    "        except Exception as e:\n",
    "            wait_time = (attempt + 1) * 5  # Increasing backoff\n",
    "            logger.error(f\"Error getting category redirect info: {str(e)}\")\n",
    "            time.sleep(wait_time)"
   ],
   "id": "5843b3e347f604bc",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T13:36:00.239889Z",
     "start_time": "2025-03-24T13:35:59.854181Z"
    }
   },
   "cell_type": "code",
   "source": "get_page_redirect_title(\"Category:ဒဿနိကဗေဒနှင့်_လူ့အဖွဲ့အစည်း\")",
   "id": "b6c3fbee48eb6258",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ကဏ္ဍ:ဒဿနိကဗေဒနှင့် လူ့အဖွဲ့အစည်း'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T13:36:02.705455Z",
     "start_time": "2025-03-24T13:36:02.272768Z"
    }
   },
   "cell_type": "code",
   "source": "get_category_redirect_name(\"Category:ဒဿနိကဗေဒနှင့်_လူ့အဖွဲ့အစည်း\")",
   "id": "1cb43afe33bc3cd7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ကဏ္ဍ:ဒဿနိကဗေဒနှင့် လူ့အဖွဲ့အစည်း'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T13:36:05.574073Z",
     "start_time": "2025-03-24T13:36:05.166498Z"
    }
   },
   "cell_type": "code",
   "source": "get_category_redirect_name(\"Category:မြန်မာ့ သမိုင်း\")",
   "id": "be70a9e08ab7e960",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ကဏ္ဍ:မြန်မာ့ သမိုင်း'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T13:18:21.626451Z",
     "start_time": "2025-03-24T13:18:21.085178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_url = \"https://my.wikipedia.org/w/api.php\"\n",
    "\n",
    "params = {\n",
    "    'action': 'query',\n",
    "    'titles': \"Category:ဒဿနိကဗေဒ\",\n",
    "    'prop': 'categories',  # This specifically asks for categories\n",
    "    'cllimit': 500,        # Maximum number of categories to return\n",
    "    'format': 'json'\n",
    "}\n",
    "\n",
    "categories = []\n",
    "continuation = True\n",
    "max_retries = 5\n",
    "response = requests.get(url=base_url, params=params).json()\n",
    "print(response)\n"
   ],
   "id": "a71b83e45c524231",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batchcomplete': '', 'query': {'normalized': [{'from': 'Category:ဒဿနိကဗေဒ', 'to': 'ကဏ္ဍ:ဒဿနိကဗေဒ'}], 'pages': {'17429': {'pageid': 17429, 'ns': 14, 'title': 'ကဏ္ဍ:ဒဿနိကဗေဒ', 'categories': [{'ns': 14, 'title': 'ကဏ္ဍ:ပင်မ အကြောင်းအရာ ခွဲခြားရေးများ'}]}}}}\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T13:18:58.589894Z",
     "start_time": "2025-03-24T13:18:58.587685Z"
    }
   },
   "cell_type": "code",
   "source": "response['query']['pages']",
   "id": "d57a4e14d5d3c6b8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'17429': {'pageid': 17429,\n",
       "  'ns': 14,\n",
       "  'title': 'ကဏ္ဍ:ဒဿနိကဗေဒ',\n",
       "  'categories': [{'ns': 14, 'title': 'ကဏ္ဍ:ပင်မ အကြောင်းအရာ ခွဲခြားရေးများ'}]}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T13:40:38.958267Z",
     "start_time": "2025-03-24T13:40:38.956635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "import multiprocessing"
   ],
   "id": "9512d7675e5db6d0",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T13:42:39.304540Z",
     "start_time": "2025-03-24T13:42:38.546541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use multiprocessing Pool to process categories in parallel\n",
    "with multiprocessing.Pool(processes=10) as pool:\n",
    "    # Use tqdm to show a progress bar\n",
    "    results = list(tqdm(\n",
    "        pool.imap(get_page_redirect_title, ['Category:မြန်မာ့ သမိုင်း', \"ဒဿနိကဗေဒနှင့်_လူ့အဖွဲ့အစည်း\"]),\n",
    "        total=len(['ကဏ္ဍ:မြန်မာနိုင်ငံ၏ သမိုင်း', 'ကဏ္ဍ:ဒဿနိကဗေဒနှင့် လူ့အဖွဲ့အစည်း']),\n",
    "        desc=\"Processing categories\"\n",
    "    ))"
   ],
   "id": "19198f014d97ac84",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing categories: 100%|██████████| 2/2 [00:00<00:00,  2.84it/s]\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T13:42:42.420754Z",
     "start_time": "2025-03-24T13:42:42.418557Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "id": "de6e9520114cc017",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ကဏ္ဍ:မြန်မာနိုင်ငံ၏ သမိုင်း', 'ဒဿနိကဗေဒနှင့် လူ့အဖွဲ့အစည်း']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d0a6ffb1a997f75c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T13:35:36.241741Z",
     "start_time": "2025-03-24T13:35:35.863712Z"
    }
   },
   "cell_type": "code",
   "source": "get_page_redirect_title(\"Category:မြန်မာ့ သမိုင်း\")",
   "id": "d7e057a48c3003b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ကဏ္ဍ:မြန်မာနိုင်ငံ၏ သမိုင်း'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T13:02:42.105381Z",
     "start_time": "2025-03-24T13:02:41.770214Z"
    }
   },
   "cell_type": "code",
   "source": "get_category_members(\"ဒဿနိကဗေဒနှင့်_လူ့အဖွဲ့အစည်း\")",
   "id": "162503152b585f4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[{'pageid': 82542, 'ns': 14, 'title': 'ကဏ္ဍ:ဒဿနိကဗေဒ ပညာရေး'}]\n",
      "1\n",
      "[] ['ကဏ္ဍ:ဒဿနိကဗေဒ ပညာရေး']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], ['ကဏ္ဍ:ဒဿနိကဗေဒ ပညာရေး'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T12:56:58.690875Z",
     "start_time": "2025-03-24T12:56:57.746794Z"
    }
   },
   "cell_type": "code",
   "source": "df, subcategoreis = fetch_pages_and_subcategories(category=[\"ဒဿနိကဗေဒနှင့်_လူ့အဖွဲ့အစည်း\"])",
   "id": "4bd4b5d5a322514d",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T12:57:00.055458Z",
     "start_time": "2025-03-24T12:57:00.053371Z"
    }
   },
   "cell_type": "code",
   "source": "subcategoreis",
   "id": "81d5d2bb05b6b65b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ကဏ္ဍ:ဒဿနိကဗေဒ ပညာရေး']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9c88ca15fa91d513"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Manual Verification of Entry Page Links and Categories Count",
   "id": "2318a6e2909b7cf6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T12:04:53.653457Z",
     "start_time": "2025-03-24T12:04:53.649977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "manual_verification_links = \"\"\"\n",
    "မြန်မာနိုင်ငံ\n",
    "စီးပွားရေး • နိုင်ငံရေး • ပညာရေး • ပထဝီဝင် • ဘာသာစကားများ • ယဉ်ကျေးမှု • လူမျိုးများ • သမိုင်း • အတ္ထုပ္ပတ္တိ • အုပ်ချုပ်ရေး\n",
    "\n",
    "\n",
    "\n",
    "ကျန်းမာရေး\n",
    "ပြည်သူ့ကျန်းမာရေး • ဆေးပညာ • ဆေးများ • ရောဂါ\n",
    "\n",
    "\n",
    "ဘာသာရေး\n",
    "ခရစ်ယာန် • ဂျိန်း • ဂျူး • ဆစ်ခ် • ဗုဒ္ဓ • ဟိန္ဒူ • အစ္စလာမ်\n",
    "\n",
    "\n",
    "သိပ္ပံ\n",
    "ဇီဝဗေဒ • ဓာတုဗေဒ • နက္ခတ္တဗေဒ • ဘူမိဗေဒ • ရုက္ခဗေဒ • ရူပဗေဒ • သတ္တဗေဒ\n",
    "\n",
    "\n",
    "ယဉ်ကျေးမှု\n",
    "ကဗျာ • ဂီတ • ဂိမ်းများ • စာပေ • ဖျော်ဖြေရေး • ရုပ်ရှင် • ပန်းချီ • ပြတိုက်များ • ပွဲတော်များ • ဗိသုကာ • ဘာသာစကား • အနုပညာ • အားကစား\n",
    "\n",
    "\n",
    "နည်းပညာ\n",
    "ကွန်ပျူတာ • ဆက်သွယ်ရေး • ဆော့ဖ်ဝဲလ် • သယ်ယူပို့ဆောင်ရေး • အီလက်ထရွန်းနစ် • အင်ဂျင်နီယာ\n",
    "\n",
    "\n",
    "ပထဝီဝင်\n",
    "ဂြိုဟ် • ကမ္ဘာ • တိုက် • (တောင်အမေရိက • မြောက်အမေရိက • အာဖရိက • အာရှ • အန္တာတိက • ဥရောပ • ဩစတြေးလျ) • အိုရှန်းနီးယား • တောင်များ • နိုင်ငံများ • ပင်လယ်  • မြစ်များ • မြို့များ • ရေထု • ရေကန်များ\n",
    "\n",
    "\n",
    "လူ့အဖွဲ့အစည်း\n",
    "မနုဿဗေဒ • ပညာရေး • (ကျောင်း • ကောလိပ် • တက္ကသိုလ်) • လူ့ဘဝ • မိသားစု • သဘာဝတ္ထဗေဒ • ဒဿနပညာ • ဘာသာဗေဒ • ဥပဒေပညာ • လူမျိုး • လက်ထပ်ထိမ်းမြားခြင်း • နိုင်ငံရေး\n",
    "\n",
    "\n",
    "သမိုင်း\n",
    "ရှေးဟောင်းသုတေသနပညာ • ကျောက်ခေတ် • ရေခဲခေတ် • သံခေတ် • ရှေးဟောင်းအီဂျစ် • အမှောင်ခေတ် • အလယ်ခေတ် • ရီနေဆွန်းခေတ် • ဗြိတိသျှအင်ပါယာ • ပထမကမ္ဘာစစ် • ဒုတိယကမ္ဘာစစ် • မြန်မာ့သမိုင်း • စစ်အေးတိုက်ပွဲ\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "len([link for link in manual_verification_links.split() if link != '•'])"
   ],
   "id": "c524130f0c72a92f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T05:08:18.571180124Z",
     "start_time": "2025-03-15T12:41:54.092718Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b02846109a83131d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "25cbcec4bdbb0f1b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
